{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True, raise_on_error=True)\n",
    "nltk.download('stopwords', quiet=True, raise_on_error=True)\n",
    "nltk_stop_words = list(set(nltk.corpus.stopwords.words('english')))\n",
    "nltk_porter_stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# Files\n",
    "ROOT_FOLDERPATH = Path.cwd().parent.parent\n",
    "DATASET_FOLDERPATH = ROOT_FOLDERPATH / 'data' / 'raw'\n",
    "NEG_DATASET_FILEPATH = DATASET_FOLDERPATH / 'rt-polarity.neg'\n",
    "POS_DATASET_FILEPATH = DATASET_FOLDERPATH / 'rt-polarity.pos'\n",
    "MODEL_FILEPATH = ROOT_FOLDERPATH / 'model' / 'poc-01--bag-of-words--naive-bayes.pickle'\n",
    "\n",
    "# LABELS\n",
    "POS = 'POS'\n",
    "NEG = 'NEG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load the data\n",
    "##### Overview the file formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplistic , silly and tedious . \n",
      "\n",
      "it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "linecount = 5\n",
    "with NEG_DATASET_FILEPATH.open() as dataset:\n",
    "    head = [next(dataset) for x in range(linecount)]\n",
    "    \n",
    "print(*head, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Load the dataset to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simplistic , silly and tedious . \\n',\n",
      " \"it's so laddish and juvenile , only teenage boys could possibly find it \"\n",
      " 'funny . \\n',\n",
      " 'exploitative and largely devoid of the depth or sophistication that would '\n",
      " 'make watching such a graphic treatment of the crimes bearable . \\n',\n",
      " '[garbus] discards the potential for pathological study , exhuming instead , '\n",
      " 'the skewed melodrama of the circumstantial situation . \\n',\n",
      " 'a visually flashy but narratively opaque and emotionally vapid exercise in '\n",
      " 'style and mystification . \\n']\n"
     ]
    }
   ],
   "source": [
    "with NEG_DATASET_FILEPATH.open() as dataset_file:\n",
    "    corpus_neg = dataset_file.readlines()\n",
    "    \n",
    "with POS_DATASET_FILEPATH.open() as dataset_file:\n",
    "    corpus_pos = dataset_file.readlines()\n",
    "    \n",
    "pprint(corpus_neg[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "##### Pre-process dataset for the Bag-of-Words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['simplist', 'silli', 'tediou'],\n",
      " ['laddish', 'juvenil', 'teenag', 'boy', 'could', 'possibl', 'find', 'funni'],\n",
      " ['exploit',\n",
      "  'larg',\n",
      "  'devoid',\n",
      "  'depth',\n",
      "  'sophist',\n",
      "  'would',\n",
      "  'make',\n",
      "  'watch',\n",
      "  'graphic',\n",
      "  'treatment',\n",
      "  'crime',\n",
      "  'bearabl'],\n",
      " ['garbu',\n",
      "  'discard',\n",
      "  'potenti',\n",
      "  'patholog',\n",
      "  'studi',\n",
      "  'exhum',\n",
      "  'instead',\n",
      "  'skew',\n",
      "  'melodrama',\n",
      "  'circumstanti',\n",
      "  'situat'],\n",
      " ['visual',\n",
      "  'flashi',\n",
      "  'narr',\n",
      "  'opaqu',\n",
      "  'emot',\n",
      "  'vapid',\n",
      "  'exercis',\n",
      "  'style',\n",
      "  'mystif']]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_line_to_tokens(line):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    tokens = (token for token in tokens if token.isalpha())\n",
    "    tokens = (token for token in tokens if token not in nltk_stop_words)\n",
    "    tokens = (nltk_porter_stemmer.stem(token) for token in tokens)\n",
    "    return list(tokens)\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    return [preprocess_line_to_tokens(line) for line in corpus]\n",
    "\n",
    "tokens_dataset_neg = preprocess_corpus(corpus_neg)\n",
    "tokens_dataset_pos = preprocess_corpus(corpus_pos)\n",
    "\n",
    "pprint(tokens_dataset_neg[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select the tokens used as features in the Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movi', 'like', 'one', 'make', 'stori', 'charact', 'time', 'comedi', 'good']\n"
     ]
    }
   ],
   "source": [
    "feature_tokens_count = 3000\n",
    "def get_all_tokens_from_dataset(tokens_dataset):\n",
    "    return [word for line in tokens_dataset for word in line]  # Flatten a 2D array\n",
    "tokens_neg = get_all_tokens_from_dataset(tokens_dataset_neg)\n",
    "tokens_pos = get_all_tokens_from_dataset(tokens_dataset_pos)\n",
    "all_tokens = tokens_neg + tokens_pos\n",
    "feature_tokens = [t[0] for t in Counter(all_tokens).most_common(feature_tokens_count)]\n",
    "\n",
    "print(feature_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the dataset to make it suitable for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'affect': True,\n",
      "   'famili': True,\n",
      "   'friendship': True,\n",
      "   'medit': True,\n",
      "   'realist': True,\n",
      "   'warm': True},\n",
      "  'POS'),\n",
      " ({'edit': True,\n",
      "   'flick': True,\n",
      "   'potenti': True,\n",
      "   'pretenti': True,\n",
      "   'ruin': True,\n",
      "   'terrif': True},\n",
      "  'NEG'),\n",
      " ({'els': True, 'hey': True, 'need': True, 'shower': True}, 'POS')]\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_model_input(tokens):\n",
    "    model_input = {}\n",
    "    for token in tokens:\n",
    "        model_input[token] = (token in feature_tokens)\n",
    "    return model_input\n",
    "\n",
    "dataset = []\n",
    "dataset.extend((tokens_to_model_input(tokens), POS) for tokens in tokens_dataset_pos)\n",
    "dataset.extend((tokens_to_model_input(tokens), NEG) for tokens in tokens_dataset_neg)\n",
    "\n",
    "random.seed(834)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "training_testing_split_ratio = 0.80\n",
    "split_index = int(training_testing_split_ratio * len(dataset))\n",
    "training_dataset, testing_dataset = dataset[:split_index], dataset[split_index:]\n",
    "\n",
    "pprint(training_dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    bore = True              NEG : POS    =     18.9 : 1.0\n",
      "                 engross = True              POS : NEG    =     15.5 : 1.0\n",
      "                   intim = True              POS : NEG    =     14.9 : 1.0\n",
      "                  absorb = True              POS : NEG    =     14.2 : 1.0\n",
      "                    wast = True              NEG : POS    =     13.1 : 1.0\n",
      "               spielberg = True              POS : NEG    =     12.9 : 1.0\n",
      "            refreshingli = True              POS : NEG    =     12.9 : 1.0\n",
      "                    warm = True              POS : NEG    =     12.9 : 1.0\n",
      "                    lame = True              NEG : POS    =     12.4 : 1.0\n",
      "                 meander = True              NEG : POS    =     12.4 : 1.0\n",
      "                  stupid = True              NEG : POS    =     11.9 : 1.0\n",
      "                    dull = True              NEG : POS    =     11.8 : 1.0\n",
      "                   urban = True              POS : NEG    =     11.6 : 1.0\n",
      "                   stale = True              NEG : POS    =     11.1 : 1.0\n",
      "                mindless = True              NEG : POS    =     11.1 : 1.0\n",
      "                  routin = True              NEG : POS    =     10.7 : 1.0\n",
      "                 tiresom = True              NEG : POS    =     10.4 : 1.0\n",
      "                    joke = True              NEG : POS    =      9.7 : 1.0\n",
      "                  unless = True              NEG : POS    =      9.7 : 1.0\n",
      "                document = True              POS : NEG    =      9.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_dataset)\n",
    "\n",
    "classifier.show_most_informative_features(20)\n",
    "\n",
    "with MODEL_FILEPATH.open('wb') as model_file:\n",
    "    pickle.dump(classifier, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 75.71%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = nltk.classify.accuracy(classifier, testing_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deploy the model\n",
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEG'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_line = \"This movie was boring.\"\n",
    "\n",
    "model_input = tokens_to_model_input(preprocess_line_to_tokens(example_line))\n",
    "with MODEL_FILEPATH.open('rb') as model_file:\n",
    "    classifier = pickle.load(model_file)\n",
    "classifier.classify(model_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
